


<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Inclusive AI App</title>
    <style>
        /* Add styles here (same as your existing styles) */body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
            background: #1a1a2e;
            color: #e6e6e6;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: space-between;
        }

        header {
            background: linear-gradient(to right, #0f2027, #203a43, #2c5364);
            color: #00d4ff;
            padding: 40px 20px;
            text-align: center;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
            text-transform: uppercase;
            letter-spacing: 2px;
        }

        header h1 {
            font-size: 2.5rem;
        }

        .container {
            max-width: 1200px;
            margin: auto;
            padding: 20px;
            flex-grow: 1;
        }

        section {
            margin: 30px 0;
            padding: 20px;
            background: #21253a;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);
        }

        section h2 {
            color: #00ffab;
            font-size: 2rem;
            text-align: center;
        }

        .feature {
            margin: 15px 0;
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .feature input,
        .feature button,
        .feature textarea {
            margin: 10px 0;
            padding: 15px;
            border-radius: 5px;
            border: 1px solid #00d4ff;
            background: #2d3448;
            color: #e6e6e6;
            width: 100%;
            max-width: 500px;
        }

        button {
            background-color: #00d4ff;
            color: #1a1a2e;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: bold;
            font-size: 1.2rem;
            padding: 15px;
        }

        button:hover {
            background-color: #00ffab;
            transform: scale(1.05);
        }

        .main-buttons {
            display: flex;
            justify-content: space-around;
            margin: 50px 0;
        }

        .main-buttons button {
            font-size: 1.5rem;
            padding: 20px;
            background-color: #00d4ff;
            border: none;
            width: 30%;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
            transition: all 0.3s ease;
        }

        .main-buttons button:hover {
            background-color: #00ffab;
            transform: scale(1.1);
        }

        footer {
            text-align: center;
            padding: 15px;
            background: #0f2027;
            color: #00d4ff;
            font-size: 0.9rem;
        }

        .instructions {
            font-style: italic;
            color: #888;
            margin-top: 5px;
        }

    </style>
</head>

<body>
    <header>
        <h1>Vita AI</h1>
        <p>Empowering Accessibility Through Technology</p>
    </header>

    <div class="container">
        <!-- Main Buttons for Navigation -->
        <div class="main-buttons">
            <button onclick="navigateTo('blind'), stopAudio()">For Blind Users</button>
            <button onclick="navigateTo('mute'), stopAudio()">For Mute Users</button>
            <button onclick="navigateTo('deaf'), stopAudio()">For Deaf Users</button>
        </div>

        <!-- Blind Users Section -->
        <section id="blind" style="display:none;">
            <h2>For Blind Users</h2>
            <p>Select a function to assist you.</p>
            <div class="feature">
                <button onclick="showImageRecognition()" aria-label="Image Recognition">Image Recognition</button>
                <button onclick="showTextRecognition()" aria-label="Text Recognition">Text Recognition</button>
                <button onclick="getRealTimeLocation();hideOther()" aria-label="Get Real-Time Location">Get Real-Time Location</button>
                <p id="locationResponse"></p>
            </div>

            <!-- Image Recognition Section -->
            <div id="imageRecognitionSection" style="display:none;">
                <h3>Image Recognition</h3>
                <p>Upload an image to hear its description.</p>
                <div class="feature">
                    <input type="file" id="imageUpload" accept="image/*" aria-label="Upload an image">
                    <button onclick="analyzeImage()" aria-label="Analyze image">Analyze Image</button>
                    <button onclick="stopAudio()" aria-label="Stop Audio">Stop Audio</button>
                    <p class="instructions">Upload a clear image for the best results.</p>
                </div>
                <p id="imageResponse"></p>
            </div>

            <!-- Text Recognition Section -->
            <div id="textRecognitionSection" style="display:none;">
                <h3>Text Recognition</h3>
                <p>Extract text from an image.</p>
                <div class="feature">
                    <input type="file" id="textImageUpload" accept="image/*" aria-label="Upload an image with text">
                    <button onclick="extractTextFromImage()" aria-label="Extract Text">Extract Text</button>
                    <button onclick="stopAudio()" aria-label="Stop Audio">Stop Audio</button>
                    <p class="instructions">Upload an image with visible text for extraction.</p>
                </div>
                <p id="textResponse"></p>
            </div>
        </section>

        <!-- Mute Users Section -->
        <section id="mute" style="display:none;">
            <h2>For Mute Users</h2>
            <p>Type your message, and let the app speak it for you.</p>
            <div class="feature">
                <label for="languageSelectMute">Choose Language:</label>
                <select id="languageSelectMute">
                    <option value="en-US">English (US)</option>
                    <option value="es-ES">Spanish</option>
                    <option value="fr-FR">French</option>
                    <option value="de-DE">German</option>
                    <option value="hi-IN">Hindi</option>
                </select>
                <textarea id="textInput" placeholder="Type your message here..." aria-label="Message input"></textarea>
                <button onclick="convertTextToSpeech()" aria-label="Speak message">Speak Message</button>
                <button onclick="showMessages(), stopAudio()" aria-label="Quick Messages">Quick Messages</button>
            </div>
            <section id="preset-messages" style="display:none;" >
                <h2>Quick Messages</h2>
                <button onclick="presetSpeak('Hello, how can I help you?')">Hello, how can I help you?</button>
                <button onclick="presetSpeak('Good Morning!')">Good Morning !</button><br><br>
                <button onclick="presetSpeak('I need assistance.')">I need assistance.</button>
                <button onclick="presetSpeak('Thank you!')">Thank you!</button>
                <button onclick="presetSpeak('Excuse Me please')">Excuse Me please</button><br><br><br>
                <div>
                    <button style="align-self: center;background-color: #8fc5ce; " onclick="backMessages()" aria-label="Back">Back</button>
                </div>
            </section>
        </section>

        <!-- Deaf Users Section -->
        <section id="deaf" style="display:none;">
            <h2>For Deaf Users</h2>
            <p>Convert spoken language into text in real-time.</p>
            <div class="feature">
                  <!-- languages ke liye -->
                <label for="languageSelectDeaf">Choose Language:</label>
                <select id="languageSelectDeaf">
                    <option value="en-US">English (US)</option>
                    <option value="es-ES">Spanish</option>
                    <option value="fr-FR">French</option>
                    <option value="de-DE">German</option>
                    <option value="hi-IN">Hindi</option>
                </select>
                <button onclick="startSpeechToText()" aria-label="Start speech-to-text">Start Speech-to-Text</button>
                <button onclick="stopSpeechToText()" aria-label="Stop speech-to-text">Stop Speech-to-Text</button>
                <p id="speechOutput"></p>
            </div>
        </section>
    </div>
  <!-- names and right-->
    <footer>
        <p>&copy; 2024 Vita AI. Designed for accessibility and innovation.</p>
        <p>Created with dedication by <strong>Pratik Rajendra Nagargoje</strong>, <strong>Kalyani Balaraju Pucha</strong>, and <strong>Atharva Dattatray Damale</strong>.</p>
    </footer>

    <script>
        // Function to navigate to different sections
        function navigateTo(section) {
            document.getElementById('blind').style.display = 'none';
            document.getElementById('mute').style.display = 'none';
            document.getElementById('deaf').style.display = 'none';
            document.getElementById(section).style.display = 'block';
        }

        // Show Image Recognition section
        function showImageRecognition() {
            document.getElementById('imageRecognitionSection').style.display = 'block';
            document.getElementById('textRecognitionSection').style.display = 'none';
        }

        // Show Text Recognition section
        function showTextRecognition() {
            document.getElementById('textRecognitionSection').style.display = 'block';
            document.getElementById('imageRecognitionSection').style.display = 'none';
        }

        // Show Quick Messages section
        function showMessages() {
            document.getElementById('preset-messages').style.display = 'block';
            document.getElementById('textRecognitionSection').style.display = 'none';
        }

        function backMessages() {
            document.getElementById('mute').style.display = 'block';
            document.getElementById('preset-messages').style.display = 'none';
        }

        function hideOther() {
            document.getElementById('imageRecognitionSection').style.display = 'none';
            document.getElementById('textRecognitionSection').style.display = 'none';
        }

        // Analyze Image for Blind Users
        async function analyzeImage() {
            const language = document.getElementById("languageSelectBlind").value;
            const fileInput = document.getElementById("imageUpload");
            const responseElement = document.getElementById("imageResponse");

            if (fileInput.files.length === 0) {
                responseElement.textContent = "Please upload an image.";
                speakText("Please upload an image.", language);
                return;
            }

            // Implement API calls for multilingual support (same as original with language parameter)
        }
                // Analyze Image for Blind Users using Google Cloud Vision API
                async function analyzeImage() {
            const fileInput = document.getElementById("imageUpload");
            const responseElement = document.getElementById("imageResponse");

            if (fileInput.files.length === 0) {
                responseElement.textContent = "Please upload an image.";
                speakText("Please upload an image.");
                return;
            }

            const file = fileInput.files[0];
            const reader = new FileReader();

            reader.onload = async function () {
                const base64Image = reader.result.split(",")[1];

                const requestBody = {
                    requests: [
                        {
                            image: { content: base64Image },
                            features: [{ type: "LABEL_DETECTION", maxResults: 10 }]
                        }
                    ]
                };

                const apiKey = "AIzaSyDqdgibUUSZEr22Nr01P4oZKYI9znHrGhM"; 
                const apiUrl = `https://vision.googleapis.com/v1/images:annotate?key=${apiKey}`;

                responseElement.textContent = "Processing image...";
                speakText("Processing image...");

                try {
                    const response = await fetch(apiUrl, {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json"
                        },
                        body: JSON.stringify(requestBody)
                    });

                    if (!response.ok) {
                        throw new Error("Failed to analyze image.");
                    }

                    const data = await response.json();
                    if (data.responses[0].labelAnnotations && data.responses[0].labelAnnotations.length > 0) {
                        const labels = data.responses[0].labelAnnotations.map(label => label.description).join(', ');
                        responseElement.textContent = `Image contains: ${labels}`;
                        speakText(`The image contains: ${labels}`);
                    } else {
                        throw new Error("No recognizable objects found.");
                    }
                } catch (error) {
                    responseElement.textContent = error.message;
                    speakText(error.message);
                }
            };

            reader.readAsDataURL(file);
        }

        // Extract Text from Image for Blind Users
        async function extractTextFromImage() {
            const fileInput = document.getElementById("textImageUpload");
            const responseElement = document.getElementById("textResponse");

            if (fileInput.files.length === 0) {
                responseElement.textContent = "Please upload an image with text.";
                speakText("Please upload an image with text.");
                return;
            }

            const file = fileInput.files[0];
            const reader = new FileReader();

            reader.onload = async function () {
                const base64Image = reader.result.split(",")[1];

                const requestBody = {
                    requests: [
                        {
                            image: { content: base64Image },
                            features: [{ type: "TEXT_DETECTION", maxResults: 10 }]
                        }
                    ]
                };

                const apiKey = "AIzaSyDqdgibUUSZEr22Nr01P4oZKYI9znHrGhM";
                const apiUrl = `https://vision.googleapis.com/v1/images:annotate?key=${apiKey}`;

                responseElement.textContent = "Processing image...";
                speakText("Processing image...");

                try {
                    const response = await fetch(apiUrl, {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json"
                        },
                        body: JSON.stringify(requestBody)
                    });

                    if (!response.ok) {
                        throw new Error("Failed to fetch text detection.");
                    }

                    const data = await response.json();
                    if (data.responses[0].textAnnotations && data.responses[0].textAnnotations.length > 0) {
                        const extractedText = data.responses[0].textAnnotations[0].description;
                        responseElement.textContent = `Extracted Text: ${extractedText}`;
                        speakText(`Extracted Text: ${extractedText}`);
                    } else {
                        throw new Error("No text detected in the image.");
                    }
                } catch (error) {
                    responseElement.textContent = error.message;
                    speakText(error.message);
                }
            };

            reader.readAsDataURL(file);
        }

        // Stop Audio for Blind Users
        function stopAudio() {
            window.speechSynthesis.cancel();
        }

        // Function to get real-time location and speak it aloud
function getRealTimeLocation() {
    const responseElement = document.getElementById("locationResponse");

    // Check if Geolocation is supported
    if (!navigator.geolocation) {
        const errorMessage = "Geolocation is not supported by your browser.";
        responseElement.textContent = errorMessage;
        speakText(errorMessage);
        return;
    }

    // Start location detection
    responseElement.textContent = "Detecting location...";
    speakText("Detecting location...");

    navigator.geolocation.getCurrentPosition(
        async (position) => {
            const latitude = position.coords.latitude;
            const longitude = position.coords.longitude;

            // Use reverse geocoding to get address (using a geocoding API)
            const apiKey = "AIzaSyACjqESP9U5B44JovhYbddcID2WhKEekHU"; 
            const apiUrl = `https://maps.googleapis.com/maps/api/geocode/json?latlng=${latitude},${longitude}&key=${apiKey}`;

            try {
                const response = await fetch(apiUrl);
                if (!response.ok) {
                    throw new Error("Failed to fetch location details.");
                }

                const data = await response.json();
                if (data.status === "OK" && data.results.length > 0) {
                    const address = data.results[0].formatted_address;
                    const locationMessage = `You are currently at: ${address}`;
                    responseElement.textContent = locationMessage;
                    speakText(locationMessage);
                } else {
                    throw new Error("Unable to determine the location address.");
                }
            } catch (error) {
                responseElement.textContent = error.message;
                speakText(error.message);
            }
        },
        (error) => {
            let errorMessage = "Unable to retrieve your location.";
            if (error.code === error.PERMISSION_DENIED) {
                errorMessage = "Location access denied. Please allow location permissions.";
            } else if (error.code === error.POSITION_UNAVAILABLE) {
                errorMessage = "Location information is unavailable.";
            } else if (error.code === error.TIMEOUT) {
                errorMessage = "The request to get your location timed out.";
            }
            responseElement.textContent = errorMessage;
            speakText(errorMessage);
        }
    );
}

        // Text-to-Speech for Mute Users
        function convertTextToSpeech() {
            const language = document.getElementById("languageSelectMute").value;
            const text = document.getElementById("textInput").value.trim();
            if (!text) {
                alert("Please enter some text.");
                return;
            }
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = language;
            window.speechSynthesis.speak(utterance);
        }

        function presetSpeak(message) {
            const speech = new SpeechSynthesisUtterance(message);
            window.speechSynthesis.speak(speech);
        }

        let recognition; // Declare recognition instance globally

// Start Speech-to-Text for Deaf Users
function startSpeechToText() {
    const language = document.getElementById("languageSelectDeaf").value;
    const outputElement = document.getElementById("speechOutput");

    // Initialize SpeechRecognition only if not already initialized
    if (!recognition) {
        recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.lang = language;
        recognition.interimResults = true; // To get fast results
        recognition.continuous = true; // To keep this running continuously

        recognition.onresult = (event) => {
            const transcript = Array.from(event.results)
                .map(result => result[0].transcript)
                .join('');
            outputElement.textContent = transcript; // Update the output with the transcript
        };

        recognition.onerror = (event) => {
            console.error("Speech recognition error:", event.error);
            outputElement.textContent = `Error: ${event.error}`;
        };
    }

    recognition.start(); // Start recognition
    outputElement.textContent = "Listening for speech...";
}

// Stop Speech-to-Text
function stopSpeechToText() {
    if (recognition) {
        recognition.stop(); // Stop recognition manually
        recognition = null; // Reset the recognition instance
        document.getElementById("speechOutput").textContent = "Speech recognition stopped.";
    }
}

        // Speak Text for Blind Users
        function speakText(text, language) {
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = language;
            window.speechSynthesis.speak(utterance);
        }

    </script>
    



  <!-- our chatbot  -->

        
<script src="https://www.gstatic.com/dialogflow-console/fast/messenger/bootstrap.js?v=1"></script>
<df-messenger
  chat-icon="https:&#x2F;&#x2F;image.similarpng.com&#x2F;very-thumbnail&#x2F;2021&#x2F;05&#x2F;Colorful-circle-logo-design-illustration-on-transparent-background-PNG.png"
  chat-title="VitalAI"
  agent-id="66defd9c-f98f-4d8d-999b-c167890813f2"
  language-code="en"
></df-messenger>
</script>        
 
        
  
        
</body>

</html>

